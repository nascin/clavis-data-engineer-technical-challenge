# ==========================================
# DESAFIO TÉCNICO DATA ENGINEER - CLAVIS
# Configuração do Filebeat
# ==========================================
# Coleta dados de criptomoedas de arquivos JSON

filebeat.inputs:
  # ==========================================
  # INPUT DE PREÇOS DE CRIPTOMOEDAS
  # ==========================================
  - type: log
    enabled: true
    paths:
      - /var/log/data/crypto_prices_*.json

    # Tags para identificar a origem dos dados
    tags: ["clavis-challenge", "crypto", "coinmarketcap", "prices"]

    # Campos personalizados adicionados a cada evento
    fields:
      pipeline: crypto-prices
      data_source: coinmarketcap
      data_type: crypto_prices

    # Configurações de leitura de arquivos
    tail_files: false           # Lê arquivo completo desde o início
    ignore_older: 48h           # Ignora arquivos mais antigos que 48 horas
    close_inactive: 5m          # Fecha arquivo após 5 minutos de inatividade
    clean_inactive: 72h         # Remove estado de arquivos inativos após 72 horas

    # Formato NDJSON: cada linha é um JSON completo (não necessita multiline)
    # Arquivos são salvos em formato NDJSON pela DAG do Airflow

  # ==========================================
  # INPUT DE MÉTRICAS GLOBAIS DO MERCADO
  # ==========================================
  - type: log
    enabled: true
    paths:
      - /var/log/data/crypto_global_metrics_*.json

    # Tags para identificar métricas globais
    tags: ["clavis-challenge", "crypto", "coinmarketcap", "global-metrics"]

    # Campos personalizados para métricas globais
    fields:
      pipeline: crypto-global-metrics
      data_source: coinmarketcap
      data_type: global_metrics

    # Configurações de leitura de arquivos
    tail_files: false           # Lê arquivo completo desde o início
    ignore_older: 48h           # Ignora arquivos mais antigos que 48 horas
    close_inactive: 5m          # Fecha arquivo após 5 minutos de inatividade
    clean_inactive: 72h         # Remove estado de arquivos inativos após 72 horas

    # Formato NDJSON: cada linha é um JSON completo (não necessita multiline)
    # Arquivos são salvos em formato NDJSON pela DAG do Airflow

  # ==========================================
  # LOGS DE EXECUÇÃO DO PIPELINE (Airflow)
  # ==========================================
  - type: log
    enabled: true
    paths:
      - /var/log/data/pipeline_execution_*.json

    # Tags para identificar logs de execução
    tags: ["clavis-challenge", "pipeline", "airflow", "monitoring"]

    # Campos personalizados para logs do pipeline
    fields:
      pipeline: pipeline-metrics
      data_source: airflow
      data_type: execution_logs

    # Configurações de leitura de arquivos
    tail_files: false           # Lê arquivo completo desde o início
    ignore_older: 48h           # Ignora arquivos mais antigos que 48 horas
    close_inactive: 5m          # Fecha arquivo após 5 minutos de inatividade
    clean_inactive: 72h         # Remove estado de arquivos inativos após 72 horas

    # Formato NDJSON: cada linha é um JSON completo (não necessita multiline)
    # Arquivos são salvos em formato NDJSON pela DAG do Airflow

# ==========================================
# PROCESSADORES
# ==========================================
processors:
  # Adiciona metadados do host onde o Filebeat está executando
  - add_host_metadata:
      when.not.contains.tags: forwarded
      netinfo.enabled: false

  # Adiciona metadados de cloud (AWS, GCP, Azure, etc)
  - add_cloud_metadata: ~

  # Adiciona metadados do container Docker
  - add_docker_metadata: ~

  # Processa e valida timestamps dos eventos
  - timestamp:
      field: "@timestamp"
      layouts:
        - '2006-01-02T15:04:05Z'
        - '2006-01-02T15:04:05.999Z'
        - '2006-01-02T15:04:05.999-07:00'
      test:
        - '2019-06-22T16:33:51Z'

  # Remove eventos vazios ou sem timestamp
  - drop_event:
      when:
        or:
          - equals:
              message: ""
          - not:
              has_fields: ['@timestamp']

# ==========================================
# SAÍDA PARA LOGSTASH
# ==========================================
output.logstash:
  hosts: ["logstash:5044"]

  # Configurações de desempenho
  worker: 2                     # Número de workers para processamento paralelo
  compression_level: 3          # Nível de compressão (0-9, 3 é balanceado)
  loadbalance: true             # Distribui carga entre múltiplos hosts

  # Configurações de envio em lote
  bulk_max_size: 2048          # Tamanho máximo do lote de eventos
  timeout: 30s                  # Timeout para envio de eventos

# ==========================================
# CONFIGURAÇÃO DE LOGS
# ==========================================
logging.level: info                      # Nível de log (debug, info, warning, error)
logging.to_files: true                   # Habilita gravação em arquivo
logging.files:
  path: /var/log/filebeat                # Diretório dos logs
  name: filebeat                         # Nome base do arquivo de log
  keepfiles: 7                           # Mantém últimos 7 arquivos de log
  permissions: 0644                      # Permissões do arquivo (rw-r--r--)
  rotateeverybytes: 10485760            # Rotaciona a cada 10MB

# ==========================================
# MONITORAMENTO
# ==========================================
monitoring.enabled: false                # Desabilita monitoramento interno do Filebeat

# ==========================================
# ENDPOINT HTTP (para health checks)
# ==========================================
http.enabled: true                       # Habilita endpoint HTTP
http.host: 0.0.0.0                       # Escuta em todas as interfaces
http.port: 5066                          # Porta para health checks e métricas
